{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rludgqx2BYyh"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum, count, avg, desc, max\n",
        "spark = SparkSession.builder.appName(\"DataFrameCheat\").getOrCreate()\n",
        "# Load CSV with schema inference\n",
        "df = spark.read.csv(\"filename.csv\", header=True, inferSchema=True)\n",
        "\n",
        "# Filter records\n",
        "df.filter(col(\"Salary\") > 5000).show()\n",
        "# Group by column and aggregate\n",
        "df.groupBy(\"Department\").agg(sum(\"Salary\").alias(\"TotalSalary\")).show()\n",
        "# Max salary row\n",
        "df.orderBy(desc(\"Salary\")).limit(1).show()\n",
        "# Count of records per group\n",
        "df.groupBy(\"Genre\").agg(count(\"*\").alias(\"TotalCount\")).show()\n",
        "# Top-N sorting example\n",
        "df.orderBy(desc(\"Rating\")).select(\"Title\", \"Rating\").show(3)\n",
        "# Add new calculated column\n",
        "df = df.withColumn(\"TotalRevenue\", col(\"Quantity\") * col(\"Price\"))\n",
        "# Rename column\n",
        "df = df.withColumnRenamed(\"OldName\", \"NewName\")\n",
        "# Save output as CSV\n",
        "df.write.csv(\"output.csv\", header=True)\n",
        "\n",
        ".createOrReplaceTempView(\"table\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#RDD\n",
        "from pyspark.sql import SparkContext\n",
        "rdd = sc.textFile(\"file.txt\")\n",
        "# Parallelize list\n",
        "rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
        "# Map, Filter, Reduce\n",
        "rdd.map(lambda x: x * 2)\n",
        "rdd.filter(lambda x: x > 50)\n",
        "rdd.reduce(lambda a, b: a + b)\n",
        "# Word Count\n",
        "rdd.flatMap(lambda line: line.split())\\\n",
        "   .map(lambda word: (word, 1))\\\n",
        "   .reduceByKey(lambda a, b: a + b)\\\n",
        "   .sortBy(lambda x: x[1], ascending=False)\\\n",
        "   .collect()"
      ],
      "metadata": {
        "id": "53REMYWoBeti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Kmeans\n",
        "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
        "from pyspark.ml.clustering import KMeans\n",
        "# Load and select features\n",
        "df = spark.read.csv(\"customers.csv\", header=True, inferSchema=True)\n",
        "df = df.select(\"Annual Income (k$)\", \"Spending Score (1-100)\")\n",
        "# Assemble and Scale\n",
        "assembler = VectorAssembler(inputCols=[\"Annual Income (k$)\", \"Spending Score (1-100)\"], outputCol=\"features\")\n",
        "df_vec = assembler.transform(df)\n",
        "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n",
        "df_scaled = scaler.fit(df_vec).transform(df_vec)\n",
        "# Apply KMeans\n",
        "kmeans = KMeans(k=3, featuresCol=\"scaledFeatures\", predictionCol=\"cluster\")\n",
        "model = kmeans.fit(df_scaled)\n",
        "model.clusterCenters()\n",
        "df_clustered = model.transform(df_scaled)\n",
        "df_clustered.select(\"Annual Income (k$)\", \"Spending Score (1-100)\", \"cluster\").show()\n",
        "df_clustered.write.csv(\"customers_clustered.csv\", header=True)"
      ],
      "metadata": {
        "id": "0GAbTOPFCqJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LR\n",
        "from pyspark.ml.regression import LinearRegression\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "# Load\n",
        "df = spark.read.csv(\"houses.txt\", header=True, inferSchema=True)\n",
        "assembler = VectorAssembler(inputCols=[\"Size (sqft)\", \"Bedrooms\"], outputCol=\"features\")\n",
        "df_vec = assembler.transform(df).select(\"features\", col(\"Price ($)\").alias(\"label\"))\n",
        "# Train-Test Split\n",
        "train, test = df_vec.randomSplit([0.8, 0.2], seed=42)\n",
        "# Train Model\n",
        "lr = LinearRegression()\n",
        "model = lr.fit(train)\n",
        "# Evaluate\n",
        "results = model.evaluate(test)\n",
        "print(\"RÂ²:\", results.r2)\n",
        "print(\"RMSE:\", results.rootMeanSquaredError)\n",
        "# Predict\n",
        "predictions = model.transform(test)\n",
        "predictions.select(\"features\", \"label\", \"prediction\").show()"
      ],
      "metadata": {
        "id": "8JLxoMGTDBdg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}