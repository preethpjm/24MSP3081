{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KZ15HNN4nd6Z"
      },
      "outputs": [],
      "source": [
        "# Neural\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('your_dataset.csv')  # Replace with your file\n",
        "X = df.drop('target', axis=1).values\n",
        "y = df['target'].values\n",
        "\n",
        "# Preprocessing\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build model\n",
        "model = Sequential([\n",
        "    Dense(64, input_dim=X.shape[1], activation='relu'),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Change to softmax for multiclass\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple CNN Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Image generator\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',  # Replace with your directory\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# CNN model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(2,2),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1, activation='sigmoid')  # Use softmax for multi-class\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, validation_data=val_generator, epochs=10)\n"
      ],
      "metadata": {
        "id": "4UQilGghnjMB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom CNN Image\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Data preparation\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='training'\n",
        ")\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',\n",
        "    target_size=(64, 64),\n",
        "    batch_size=32,\n",
        "    class_mode='binary',\n",
        "    subset='validation'\n",
        ")\n",
        "\n",
        "# Model\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3, 3), activation='relu', input_shape=(64, 64, 3)),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Conv2D(64, (3, 3), activation='relu'),\n",
        "    MaxPooling2D(pool_size=(2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')  # Use softmax for multiclass\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(train_generator, validation_data=val_generator, epochs=10)"
      ],
      "metadata": {
        "id": "jzhLYHdgoHii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Transfer Learning with VGG16, VGG19, ResNet50, InceptionV3\n",
        "\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, InceptionV3\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def build_model(base_model_class, input_shape=(224, 224, 3), num_classes=2):\n",
        "    base_model = base_model_class(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(1024, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax' if num_classes > 2 else 'sigmoid')(x)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(), loss='categorical_crossentropy' if num_classes > 2 else 'binary_crossentropy', metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Example usage\n",
        "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'  # or 'binary'\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    'path_to_images',\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "model = build_model(VGG16, num_classes=train_generator.num_classes)\n",
        "model.fit(train_generator, validation_data=val_generator, epochs=5)\n"
      ],
      "metadata": {
        "id": "7HPGxLxNoXhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM GRU RNN\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, SimpleRNN, GRU, Dense\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv('stock_prices.csv')  # Replace with your CSV\n",
        "data = df['Close'].values.reshape(-1, 1)\n",
        "\n",
        "# Normalize\n",
        "scaler = MinMaxScaler()\n",
        "data_scaled = scaler.fit_transform(data)\n",
        "\n",
        "# Sequence generation\n",
        "def create_sequences(data, seq_len=60):\n",
        "    X, y = [], []\n",
        "    for i in range(seq_len, len(data)):\n",
        "        X.append(data[i - seq_len:i])\n",
        "        y.append(data[i])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "X, y = create_sequences(data_scaled)\n",
        "X_train, y_train = X[:int(0.8*len(X))], y[:int(0.8*len(X))]\n",
        "X_test, y_test = X[int(0.8*len(X)):], y[int(0.8*len(X)):]\n",
        "\n",
        "# Choose model\n",
        "model_type = 'LSTM'  # Change to 'RNN' or 'GRU'\n",
        "model = Sequential()\n",
        "if model_type == 'LSTM':\n",
        "    model.add(LSTM(50, input_shape=(X.shape[1], 1)))\n",
        "elif model_type == 'RNN':\n",
        "    model.add(SimpleRNN(50, input_shape=(X.shape[1], 1)))\n",
        "elif model_type == 'GRU':\n",
        "    model.add(GRU(50, input_shape=(X.shape[1], 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Predict\n",
        "predicted = model.predict(X_test)\n",
        "predicted_prices = scaler.inverse_transform(predicted)\n",
        "actual_prices = scaler.inverse_transform(y_test)\n",
        "\n",
        "plt.plot(actual_prices, label='Actual')\n",
        "plt.plot(predicted_prices, label='Predicted')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFCWBGcBogCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next word pred\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample text\n",
        "text = \"Deep learning models like LSTM are useful for text prediction tasks such as next word prediction.\"\n",
        "\n",
        "# Tokenize\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "\n",
        "# Create sequences\n",
        "sequences = []\n",
        "for i in range(1, len(sequence)):\n",
        "    seq = sequence[:i+1]\n",
        "    sequences.append(seq)\n",
        "\n",
        "# Prepare data\n",
        "max_seq_len = max(len(seq) for seq in sequences)\n",
        "X = np.array([np.pad(seq[:-1], (max_seq_len - len(seq[:-1]), 0)) for seq in sequences])\n",
        "y = to_categorical([seq[-1] for seq in sequences], num_classes=len(tokenizer.word_index)+1)\n",
        "\n",
        "# Build model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=10, input_length=max_seq_len-1))\n",
        "model.add(LSTM(50))\n",
        "model.add(Dense(len(tokenizer.word_index)+1, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=200, verbose=0)\n",
        "\n",
        "# Predict next word\n",
        "input_text = \"deep learning models\"\n",
        "encoded = tokenizer.texts_to_sequences([input_text])[0]\n",
        "padded = np.pad(encoded, (max_seq_len - 1 - len(encoded), 0))\n",
        "predicted_index = np.argmax(model.predict(np.array([padded])), axis=-1)[0]\n",
        "\n",
        "for word, index in tokenizer.word_index.items():\n",
        "    if index == predicted_index:\n",
        "        print(f\"Next word prediction: {word}\")\n",
        "        break\n"
      ],
      "metadata": {
        "id": "vcNIUdVTorT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Vanilla ae\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.\n",
        "x_test = x_test.astype(\"float32\") / 255.\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "x_test = x_test.reshape((len(x_test), 784))\n",
        "\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(64, activation='relu')(input_img)\n",
        "decoded = Dense(784, activation='sigmoid')(encoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test, x_test))\n",
        "\n",
        "decoded_imgs = autoencoder.predict(x_test)\n",
        "plt.imshow(decoded_imgs[0].reshape(28, 28), cmap='gray')\n",
        "plt.title(\"Reconstructed Image\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sVAf19HwozR3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Deep ae\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Dense\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype(\"float32\") / 255.\n",
        "x_test = x_test.astype(\"float32\") / 255.\n",
        "x_train = x_train.reshape((len(x_train), 784))\n",
        "x_test = x_test.reshape((len(x_test), 784))\n",
        "\n",
        "input_img = Input(shape=(784,))\n",
        "encoded = Dense(512, activation='relu')(input_img)\n",
        "encoded = Dense(128, activation='relu')(encoded)\n",
        "encoded = Dense(64, activation='relu')(encoded)\n",
        "\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(512, activation='relu')(decoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train, x_train, epochs=20, batch_size=256, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "dC0YNxe2o37l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Convolution AE\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "\n",
        "(x_train, _), (x_test, _) = mnist.load_data()\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))\n",
        "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))\n",
        "\n",
        "input_img = Input(shape=(28, 28, 1))\n",
        "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
        "x = MaxPooling2D((2, 2), padding='same')(x)\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
        "encoded = MaxPooling2D((2, 2), padding='same')(x)\n",
        "\n",
        "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
        "x = UpSampling2D((2, 2))(x)\n",
        "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
        "\n",
        "autoencoder = Model(input_img, decoded)\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')\n",
        "autoencoder.fit(x_train, x_train, epochs=10, batch_size=128, shuffle=True, validation_data=(x_test, x_test))"
      ],
      "metadata": {
        "id": "LRtRrnVpo8-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q learning\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Reward matrix R\n",
        "R = np.array([\n",
        "    [-1,  0, -1,  0, -1, -1, -1],\n",
        "    [ 0, -1,100, -1,  0, -1, -1],\n",
        "    [-1, -1,-1, 100, -1,  0, -1],\n",
        "    [ 0, -1,  0, -1, -1, -1,  0],\n",
        "    [-1,  0, -1, -1, -1,  0, -1],\n",
        "    [-1, -1,  0, -1,  0, -1,  0],\n",
        "    [-1, -1,100, -1,  0, -1, -1]\n",
        "])\n",
        "\n",
        "# Initialize Q-matrix\n",
        "Q = np.zeros_like(R)\n",
        "\n",
        "# Parameters\n",
        "gamma = 0.8  # discount factor\n",
        "episodes = 1000\n",
        "\n",
        "for _ in range(episodes):\n",
        "    state = random.randint(0, 6)\n",
        "    while True:\n",
        "        # Get valid actions from current state\n",
        "        actions = np.where(R[state] != -1)[0]\n",
        "        action = random.choice(actions)\n",
        "\n",
        "        next_state = action\n",
        "        max_q = max(Q[next_state])\n",
        "\n",
        "        # Bellman update\n",
        "        Q[state][action] = R[state][action] + gamma * max_q\n",
        "\n",
        "        state = next_state\n",
        "        if state == 2 or state == 3:\n",
        "            break  # terminal states\n",
        "\n",
        "print(\"Trained Q-matrix:\")\n",
        "print(Q / np.max(Q) * 100)  # normalize\n"
      ],
      "metadata": {
        "id": "ajIsJDzsMT4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V(s) = maxₐ [R(s, a) + γ * V(s′)]:\n",
        "Bellman Optimality Equation – best expected value of all possible actions.\n",
        "\n",
        "Q(s, a): Quality of taking action a in state s.\n",
        "R: Immediate reward.\n",
        "γ: Discount factor.\n",
        "α: Learning rate.\n",
        "\n",
        "Q(s, a) ← Q(s, a) + α [R + γ * max Q(s′, a′) – Q(s, a)]"
      ],
      "metadata": {
        "id": "D3ZkCD6qNAcf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}